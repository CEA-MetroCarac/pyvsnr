{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading + Loading Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading w/ skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Clean tests\n",
    "\n",
    "from skimage.io import imread\n",
    "import glob\n",
    "import time\n",
    "\n",
    "image_paths = glob.glob('/nobackup/kp276129/test/1*/slice_010*') # 10 4k tif\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "imgs = [imread(img) for img in image_paths]\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(f\"Time taken to read {len(image_paths)} images: {t2 - t1} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading w/ nvimagecodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia import nvimgcodec\n",
    "import time\n",
    "import glob\n",
    "\n",
    "params = nvimgcodec.DecodeParams(color_spec=nvimgcodec.ColorSpec.UNCHANGED, allow_any_depth=True)\n",
    "dec = nvimgcodec.Decoder()\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "imgs=dec.read(glob.glob('/nobackup/kp276129/test/1*/slice_010*'), params)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Time taken to read images: {t2 - t1} seconds\")\n",
    "# print(img.__cuda_array_interface__)\n",
    "# plt.imshow(img.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading into GPU (CuPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "imgs_cp = []\n",
    "\n",
    "# Converting 10 images to CuPy arrays\n",
    "t3 = time.perf_counter()\n",
    "for img in imgs:\n",
    "    imgs_cp.append(cp.asarray(img).squeeze())\n",
    "t4 = time.perf_counter()\n",
    "\n",
    "print(f\"Time taken to convert {len(imgs)} images to CuPy arrays: {t4 - t3} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing vnsr2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.pyvsnr import vsnr2d\n",
    "\n",
    "filters=[{'name':'Dirac', 'noise_level':0.35}]\n",
    "\n",
    "# Time to apply vsnr2d to 10 images one by one\n",
    "t5 = time.perf_counter()\n",
    "for img in imgs_cp:\n",
    "    vsnr2d(img, filters)\n",
    "t6 = time.perf_counter()\n",
    "\n",
    "print(f\"Time taken to apply vsnr2d to {len(imgs)} images: {t6 - t5} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyvsnr Average Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.pyvsnr import vsnr2d\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "filters=[{'name':'Dirac', 'noise_level':0.35}]\n",
    "img = cp.random.rand(2048, 2048).astype(cp.float32)\n",
    "nit=200\n",
    "\n",
    "# Calculatin average time for 100 images\n",
    "t1 = time.perf_counter()\n",
    "for i in range(nit):\n",
    "    vsnr2d(img, filters)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "# print average\n",
    "print(f\"Average time to apply vsnr2d: {(t2-t1)/nit} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyvsnr Batch Average Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from pyvsnr.vsnr2d import vsnr2d\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "\n",
    "def batch_generator(images, batch_size):\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        yield images[i:i+batch_size]\n",
    "\n",
    "\n",
    "nb_img = 200\n",
    "batch_size = 10\n",
    "filters=[{'name':'Dirac', 'noise_level':0.35}]\n",
    "imgs = cp.random.rand(nb_img, 2048, 2048).astype(cp.float32)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "for batch in batch_generator(imgs, batch_size):\n",
    "    vsnr2d(batch, filters)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(f\"Average time to apply vsnr2d_batch: {(t2-t1)/nb_img} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "activities = ['Reading', 'Loading', 'Processing']\n",
    "\n",
    "time_taken = [t2-t1, t4-t3, t6-t5]\n",
    "\n",
    "# Create a pie chart\n",
    "plt.pie(time_taken, labels=activities, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Add title and legend\n",
    "plt.title('Time Distribution')\n",
    "plt.legend(activities, loc=\"best\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyvsnr VRAM Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cupy as cp\n",
    "sys.path.append('..')\n",
    "from src.pyvsnr import vsnr2d\n",
    "\n",
    "filters=[{'name':'Dirac', 'noise_level':0.35}]\n",
    "nb_img = 1\n",
    "\n",
    "for _ in range(nb_img):\n",
    "    img = cp.random.rand(4224, 4224)\n",
    "    vsnr2d(img, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyvsnr VRAM Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.pyvsnr import vsnr2d\n",
    "import cupy as cp\n",
    "\n",
    "%mprun -f vsnr2d vsnr2d(cp.random.rand(4224, 4224), [{'name':'Dirac', 'noise_level':0.35}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VRAM Usage for Batch Processing FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def image_generator(nb_img, batch_size):\n",
    "    for i in range(0, nb_img, batch_size):\n",
    "        batch_imgs = cp.random.rand(batch_size, 4224, 4224, dtype=cp.float32)\n",
    "        yield batch_imgs\n",
    "\n",
    "batch_size = 2\n",
    "nb_img = 100\n",
    "\n",
    "# Processing simple FFT\n",
    "for batch in image_generator(nb_img, batch_size):\n",
    "    imgs_corr = cp.fft.fft2(batch)\n",
    "\n",
    "batch_sizes = [1,2,10,20,50]\n",
    "vram_usage = [1215,1963,7947,15437,37899]\n",
    "\n",
    "# Calculate VRAM usage per image\n",
    "for i in range(1, len(batch_sizes)):\n",
    "    vram_per_image = (vram_usage[i] - vram_usage[i-1]) / (batch_sizes[i] - batch_sizes[i-1])\n",
    "    print(vram_per_image)\n",
    "\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('VRAM Usage (MB)')\n",
    "plt.plot(batch_sizes, vram_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyvsnr Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pstats\n",
    "import cProfile\n",
    "import io\n",
    "\n",
    "sys.path.append('../')\n",
    "import src.pyvsnr as pyvsnr\n",
    "import cupy as cp\n",
    "\n",
    "img = cp.random.rand(4200, 4200)\n",
    "filters=[{'name':'Dirac', 'noise_level':0.35}]\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "pyvsnr.vsnr2d_single(img, filters)\n",
    "\n",
    "pr.disable()\n",
    "s = io.StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats('time')\n",
    "ps.print_stats()\n",
    "\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch vs Individual Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cupy as cp\n",
    "\n",
    "def image_generator(nb_img, batch_size):\n",
    "    for i in range(0, nb_img, batch_size):\n",
    "        batch_imgs = cp.random.rand(batch_size, 4224, 4224, dtype=cp.float32)\n",
    "\n",
    "        # cp.fft.config.get_plan_cache().clear()\n",
    "        # cp.get_default_memory_pool().free_all_blocks()\n",
    "        # cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "\n",
    "        yield batch_imgs\n",
    "\n",
    "batch_size = 20\n",
    "nb_img = 200\n",
    "\n",
    "# Need to measure one method at a time otherwise the first is faster\n",
    "\n",
    "\n",
    "# Initialize an empty array for the results\n",
    "# imgs_corr = cp.zeros((nb_img, 4224, 4224), dtype=cp.complex64)\n",
    "\n",
    "t7 = time.perf_counter()\n",
    "# Process the images in batches\n",
    "for batch in image_generator(nb_img, batch_size):\n",
    "    # imgs_corr[i*batch_size:(i+1)*batch_size] = cp.fft.fft2(batch)\n",
    "    cp.fft.fft2(batch)\n",
    "t8 = time.perf_counter()\n",
    "\n",
    "print(f\"Time taken to apply FFT to {nb_img} images in batches: {t8 - t7} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cupy as cp\n",
    "\n",
    "nb_img = 200\n",
    "img = cp.random.rand(4224, 4224, dtype=cp.float32)\n",
    "\n",
    "t9 = time.perf_counter()\n",
    "# Process the images individually\n",
    "for i in range(nb_img):\n",
    "        # imgs_corr_2[i*batch_size+j] = cp.fft.fft2(img)\n",
    "        cp.fft.fft2(img)\n",
    "t10 = time.perf_counter()\n",
    "\n",
    "print(f\"Time taken to apply FFT to {nb_img} images individually: {t10 - t9} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different batch sizes for Pyvsnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pyvsnr.vsnr2d import vsnr2d\n",
    "import matplotlib.pyplot as plt   \n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "def batch_generator(images, batch_size):\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        yield cp.stack(images[i:i+batch_size])\n",
    "\n",
    "filters=[{'name':'Dirac', 'noise_level':0.35}]\n",
    "img = cp.random.rand(2048, 2048).astype(cp.float32)\n",
    "\n",
    "batch_sizes = [5,10,15,20,25]\n",
    "times = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    t1 = time.perf_counter()\n",
    "    for batch in batch_generator([img]*200, batch_size):\n",
    "        vsnr2d(batch, filters)\n",
    "    t2 = time.perf_counter()\n",
    "    times.append((t2-t1)/200)\n",
    "\n",
    "    cp.fft.config.get_plan_cache().clear()\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.plot(batch_sizes, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multrithreading on pyvsnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from multiprocessing import Pool\n",
    "sys.path.append('..')\n",
    "from src.pyvsnr import vsnr2d\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "filters=[{'name':'Dirac', 'noise_level':0.35}]\n",
    "img = np.random.rand(2048, 2048) # CuPy usage leads to CUDA initialization error\n",
    "nit=200\n",
    "\n",
    "def task(i):\n",
    "    vsnr2d(img, filters,algo=\"cuda\")\n",
    "\n",
    "# Calculating average time for 100 images\n",
    "t1 = time.perf_counter()\n",
    "with Pool(10) as pool:\n",
    "    pool.map(task, range(nit))\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "# print average\n",
    "print(f\"Average time to apply vsnr2d: {(t2-t1)/nit} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imread vs imread_collection : Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imread_collection\n",
    "import glob\n",
    "import time\n",
    "\n",
    "image_paths = glob.glob('/nobackup/kp276129/test/1*/*')\n",
    "\n",
    "# Method 1 : Read with imread_collection\n",
    "t1 = time.perf_counter()\n",
    "imgs1 = list(imread_collection(image_paths))\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Time taken to read images with imread_collection: {t2 - t1} seconds\")\n",
    "\n",
    "# Method 2 : Read with imread\n",
    "t3 = time.perf_counter()\n",
    "imgs2 = [imread(path) for path in image_paths]\n",
    "t4 = time.perf_counter()\n",
    "print(f\"Time taken to read images with imread: {t4 - t3} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nvimagecodec doesn't match the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia import nvimgcodec\n",
    "import glob\n",
    "import time\n",
    "import cupy as cp\n",
    "from matplotlib import pyplot as plt\n",
    "from tifffile import imread\n",
    "\n",
    "image_paths = glob.glob('/nobackup/kp276129/test/1*/*') # 10 4k tif\n",
    "data_list = []\n",
    "\n",
    "# Read 100 images\n",
    "t1 = time.perf_counter()\n",
    "params = nvimgcodec.DecodeParams(color_spec=nvimgcodec.ColorSpec.UNCHANGED, allow_any_depth=True)\n",
    "dec = nvimgcodec.Decoder()\n",
    "imgs = dec.read(image_paths, params)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Time taken to read {len(image_paths)} images: {t2 - t1} seconds\")\n",
    "\n",
    "img=dec.read(image_paths[0], params)\n",
    "# print(img1.__cuda_array_interface__)\n",
    "img=cp.asarray(img).squeeze().get()\n",
    "print(img.shape, img.max(), img.dtype)\n",
    "plt.figure()\n",
    "plt.imshow(img) \n",
    "\n",
    "img0 = imread(image_paths[0])\n",
    "print(img0.shape, img0.max(), img0.dtype)\n",
    "plt.figure()\n",
    "plt.imshow(img0)\n",
    "print(img0.max())\n",
    "assert cp.allclose(img0, img), \"The images do not match\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Reproducible Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "from nvidia import nvimgcodec\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "\n",
    "def sh_info(img, header=\"\"):\n",
    "    print(header)\n",
    "    print(f\"\\tShape: {img.shape}\")\n",
    "    print(f\"\\tMin value: {img.min()}\")\n",
    "    print(f\"\\tMax value: {img.max()}\")\n",
    "    print(f\"\\tData type: {img.dtype}\")\n",
    "\n",
    "# Creating a random 16-bit image\n",
    "arr = np.random.random((4096, 4096))\n",
    "arr *= 60000\n",
    "arr[100:400, 100:400] = 61000\n",
    "arr = arr.astype(np.uint16)\n",
    "sh_info(arr, \"Original Image\")\n",
    "\n",
    "# Writing the image to a tif file and reading it back\n",
    "imwrite('test.tif', arr, dtype=np.uint16)\n",
    "img0 = imread('test.tif')\n",
    "sh_info(img0, \"tifffile Decoded Image\")\n",
    "\n",
    "# Decode the image using nvimgcodec\n",
    "params = nvimgcodec.DecodeParams(color_spec=nvimgcodec.ColorSpec.UNCHANGED, allow_any_depth=True)\n",
    "dec = nvimgcodec.Decoder()\n",
    "img_decoded = dec.read('test.tif', params)\n",
    "\n",
    "# Convert the decoded image to a CuPy array\n",
    "cp_img = cp.asarray(img_decoded).get()\n",
    "sh_info(cp_img, \"nvImageCodec Decoded Image\")\n",
    "\n",
    "# Create a figure with 3 subplots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(arr)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"tifffile Decoded Image\")\n",
    "plt.imshow(img0)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('nvImageCodec Decoded Image')\n",
    "plt.imshow(cp_img)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Reproducible Example #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia import nvimgcodec\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import cupy as cp\n",
    "\n",
    "def sh_info(img):\n",
    "    print(f\"\\tShape: {img.shape}\")\n",
    "    print(f\"\\tMin value: {img.min()}\")\n",
    "    print(f\"\\tMax value: {img.max()}\")\n",
    "    print(f\"\\tData type: {img.dtype}\")\n",
    "    \n",
    "img_pth = './slice_00101_z=1.0988um.tif'\n",
    "\n",
    "params = nvimgcodec.DecodeParams(color_spec=nvimgcodec.ColorSpec.UNCHANGED, allow_any_depth=True)\n",
    "dec = nvimgcodec.Decoder()\n",
    "\n",
    "# Loading with nvImageCodec\n",
    "img=dec.read(img_pth, params)\n",
    "img=cp.asarray(img).squeeze().get()\n",
    "\n",
    "# Loading with skimage\n",
    "img1 = imread(img_pth)\n",
    "\n",
    "# Expected Output\n",
    "print(\"skimage:\")\n",
    "sh_info(img1)\n",
    "plt.title('Expected Image (skimage)')\n",
    "plt.imshow(img1)\n",
    "\n",
    "# Actual Output\n",
    "print(\"nvImageCodec:\")\n",
    "sh_info(img)\n",
    "plt.figure()\n",
    "plt.title('Image (nvImageCodec)')\n",
    "plt.imshow(img) \n",
    "\n",
    "assert cp.allclose(img1, img), \"The images do not match\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use streams to overlap data transfer and computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from tifffile import imread\n",
    "import glob\n",
    "import time\n",
    "from cupyx.scipy.ndimage import gaussian_filter, sobel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# List of image paths\n",
    "image_paths = glob.glob('/nobackup/kp276129/test/2*/*')\n",
    "num_img = 100\n",
    "batch_size = 10\n",
    "\n",
    "def process_image(image):\n",
    "    # Apply a Gaussian blur\n",
    "    blurred = gaussian_filter(image, sigma=1)\n",
    "\n",
    "    # Apply a Sobel filter\n",
    "    filtered = sobel(blurred)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# Function to load a batch of images\n",
    "def load_batch(paths):\n",
    "    return [cp.array(imread(path)) for path in paths]\n",
    "\n",
    "# Function to process a batch of images\n",
    "def process_batch(batch):\n",
    "    return [process_image(image) for image in batch]\n",
    "\n",
    "# Create a ThreadPoolExecutor with more workers for better concurrency\n",
    "executor = ThreadPoolExecutor(max_workers=1)\n",
    "\n",
    "# Load the first batch of images\n",
    "batch = load_batch(image_paths[:batch_size])\n",
    "next_batch = None\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "for i in range(0, num_img, batch_size):\n",
    "    # Process the current batch in the default stream\n",
    "    result = process_batch(batch)\n",
    "\n",
    "    # In a separate thread, load the next batch of images\n",
    "    if i + batch_size < num_img:\n",
    "        future = executor.submit(load_batch, image_paths[i+batch_size:i+2*batch_size])\n",
    "        batch = future.result() # Wait for the next batch to be loaded\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Time taken with streams: {t2 - t1} seconds\")\n",
    "\n",
    "t3 = time.perf_counter()\n",
    "for i in range(0, num_img, batch_size):\n",
    "    batch = load_batch(image_paths[i:i+batch_size])\n",
    "    result = process_batch(batch)\n",
    "t4 = time.perf_counter()\n",
    "print(f\"Time taken without streams: {t4 - t3} seconds\")\n",
    "\n",
    "# TODO Slower with threads ? Loading images is faster than creating a thread ?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
